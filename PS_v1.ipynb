{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import fitz  \n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get arXiv Paper URLs\n",
    "def get_arxiv_paper_urls(query, max_results=10):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    paper_urls = []\n",
    "    for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "        paper_url = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "        paper_urls.append(paper_url)\n",
    "    \n",
    "    return paper_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract Paper Metadata\n",
    "def extract_paper_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.find(\"meta\", {\"name\": \"citation_title\"})\n",
    "    title = title_tag[\"content\"] if title_tag else \"Title not found\"\n",
    "    \n",
    "    abstract_tag = soup.find(\"blockquote\", {\"class\": \"abstract\"})\n",
    "    abstract_text = abstract_tag.text.replace(\"Abstract: \", \"\").strip() if abstract_tag else \"Abstract not found\"\n",
    "    \n",
    "    pdf_url = url.replace(\"abs\", \"pdf\") + \".pdf\"\n",
    "    \n",
    "    return {\"title\": title, \"abstract\": abstract_text, \"pdf_url\": pdf_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download and Extract PDF\n",
    "def download_and_extract_pdf(pdf_url):\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf_filename = pdf_url.split(\"/\")[-1]\n",
    "    with open(pdf_filename, \"wb\") as pdf_file:\n",
    "        pdf_file.write(response.content)\n",
    "    \n",
    "    doc = fitz.open(pdf_filename)\n",
    "    full_text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        full_text += page.get_text(\"text\")\n",
    "    doc.close()\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_papers_and_add_to_rag(query, max_results=5):\n",
    "    # Fetch arXiv paper URLs based on query\n",
    "    urls = get_arxiv_paper_urls(query, max_results=max_results)\n",
    "    \n",
    "    # Initialize a list to store documents\n",
    "    docs = []\n",
    "    \n",
    "    # Extract content and download each paper\n",
    "    for url in urls:\n",
    "        content = extract_paper_content(url)\n",
    "        print(f\"Processing: {content['title']}\")\n",
    "        \n",
    "        # Download and extract full paper content (PDF)\n",
    "        full_content = download_and_extract_pdf(content[\"pdf_url\"])\n",
    "        print(f\"Downloaded and extracted paper content, length: {len(full_content)}\")\n",
    "        \n",
    "        # Store title and content in a dictionary for processing\n",
    "        docs.append({\"title\": content[\"title\"], \"page_content\": full_content})\n",
    "    \n",
    "    # Step 5: Prepare the documents for embedding and chunking\n",
    "    # Convert docs into a list of `Document` objects\n",
    "    formatted_docs = [\n",
    "        Document(page_content=doc[\"page_content\"], metadata={\"title\": doc[\"title\"]}) for doc in docs\n",
    "    ]\n",
    "    \n",
    "    # Use RecursiveCharacterTextSplitter to chunk the document content\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(formatted_docs)\n",
    "    \n",
    "    # Step 6: Embedding model and vectorstore setup\n",
    "    embed = OllamaEmbeddings(model=\"all-minilm\")\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embed)\n",
    "    \n",
    "    # Step 7: RAG Chain setup\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    # prompt = load_prompt(\"rlm/rag-prompt\")  # Load the RAG prompt from a source or file\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert in academic research. Answer questions based on the following academic paper context.\",\n",
    "            ),\n",
    "            (\"user\", \"Here is the academic context: {context}\"),\n",
    "            (\"user\", \"Now answer the following question: {question}\"),\n",
    "            # MessagesPlaceholder(variable_name=\"messages\"),  # Placeholder for additional user messages\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOllama()  # Load your chosen language model here\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Define the RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Lecture Notes: Neural Network Architectures\n",
      "Downloaded and extracted paper content, length: 95012\n",
      "Processing: Self-Organizing Multilayered Neural Networks of Optimal Complexity\n",
      "Downloaded and extracted paper content, length: 22593\n",
      "Processing: Neural Network Processing Neural Networks: An efficient way to learn higher order functions\n",
      "Downloaded and extracted paper content, length: 7378\n"
     ]
    }
   ],
   "source": [
    "# Example: Use the function to process papers related to \"neural networks\"\n",
    "rag_chain = process_papers_and_add_to_rag(query=\"neural networks\", max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the paper \"Neural Network Processing Neural Networks: An Efficient Way to Learn Higher Order Functions\" by Firat Tuna, there are several latest advances in neural networks that are being explored:\n",
      "\n",
      "1. Meta-parameterized neural networks: The paper introduces a new class of neural networks called \"meta parameterized neural networks,\" which can generate neural networks using other neural networks as inputs. This allows for more flexible and efficient generation of neural networks.\n",
      "2. Neural network processing neural networks: The paper proposes a new architecture called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. This enables neural networks to represent and process rich structures more efÔ¨Åciently.\n",
      "3. Approximation capabilities of multilayer feedforward networks: The paper references a study by Kurt Hornik that demonstrates the approximation capabilities of multilayer feedforward networks, which are a type of neural network.\n",
      "4. HyperNetworks: The paper mentions the work of David Ha, Andrew Dai, and Quoc V. Le on HyperNetworks, which is a class of neural networks that can learn to perform various tasks, such as image classification and language modeling, by composing simpler neural networks called \"building blocks.\"\n",
      "5. Unrolled recurrent neural networks: The paper compares the architecture of feedforward neural networks (FNNs) and recurrent neural networks (RNNs), highlighting the ability of RNNs to retain a state that can represent information from an arbitrarily long context window. This suggests that RNNs may be more effective in certain tasks that require processing sequential data.\n",
      "\n",
      "Overall, these advances in neural networks demonstrate the growing interest in developing new architectures and techniques for improving the efficiency and effectiveness of neural network models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_prompt = \"What are the latest advances in neural networks?\"\n",
    "\n",
    "# query to the RAG chain\n",
    "result = rag_chain.invoke(user_prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
